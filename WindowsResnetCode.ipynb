{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import normalize\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.optimizers import Adam\n",
    "import glob\n",
    "\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "print(tf.__version__)\n",
    "\n",
    "# Check if TensorFlow is running on GPU\n",
    "if tf.test.gpu_device_name():\n",
    "    print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TensorFlow\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ... (Same code for loading and preprocessing images and masks)\n",
    "#Load images and masks in order so they match\n",
    "image_directory = r\"c:\\Users\\richch\\Desktop\\ModSOCPristineTiffs\\SOC\"\n",
    "mask_directory = r\"c:\\Users\\richch\\Desktop\\ModSOCPristineTiffs\\MaskSOC\"\n",
    "\n",
    "num_images=500\n",
    "SIZE = 128\n",
    "\n",
    "image_names = glob.glob(r\"c:\\Users\\richch\\Desktop\\ModSOCPristineTiffs\\SOC\\*.tif\")\n",
    "#returns list of files or folders that matches the path specified in the pathname argument\n",
    "print(image_names)\n",
    "\n",
    "image_names.sort()\n",
    "\n",
    "image_names_subset = image_names[0:num_images]\n",
    "     \n",
    "images = [cv2.imread(img, 0) for img in image_names_subset]\n",
    "     \n",
    "\n",
    "image_dataset = np.array(images)\n",
    "image_dataset = np.expand_dims(image_dataset, axis=-1)\n",
    "\n",
    "mask_names = glob.glob(r\"c:\\Users\\richch\\Desktop\\ModSOCPristineTiffs\\MaskSOC\\*.tif\")\n",
    "mask_names.sort()\n",
    "mask_names_subset = mask_names[0:num_images]\n",
    "masks = [cv2.imread(mask, 0) for mask in mask_names_subset]\n",
    "mask_dataset = np.array(masks)\n",
    "mask_dataset = np.expand_dims(mask_dataset, axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "# print(\"Image data shape is: \", image_dataset.shape)\n",
    "# print(\"Mask data shape is: \", mask_dataset.shape)\n",
    "# print(\"Max pixel value in image is: \", image_dataset.max())\n",
    "# print(\"Labels in the mask are : \", np.unique(mask_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize images\n",
    "image_dataset = image_dataset /255.  #Can also normalize or scale using MinMax scaler\n",
    "#Do not normalize masks, just rescale to 0 to 1.\n",
    "mask_dataset = mask_dataset /255.  #PIxel values will be 0 or 1\n",
    "     \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(image_dataset, mask_dataset, test_size = 0.20, random_state = 42)\n",
    "\n",
    "\n",
    "#Sanity check\n",
    "import random\n",
    "\n",
    "image_number = random.randint(0, len(X_train)-1)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(121)\n",
    "plt.imshow(X_train[image_number,:,:,0], cmap='gray')\n",
    "plt.subplot(122)\n",
    "plt.imshow(y_train[image_number,:,:,0], cmap='gray')\n",
    "plt.show()    \n",
    "#plt or Matplotlib is a plotting library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import UpSampling2D\n",
    "from keras.layers import Input, Concatenate, Conv2D, Conv2DTranspose, Activation, Lambda\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define helper function for the decoder block\n",
    "# Block converts a 2D image into an array\n",
    "def decoder_block(input, skip_features, num_filters):\n",
    "    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(input)\n",
    "    x = Concatenate()([x, skip_features])\n",
    "    x = Conv2D(num_filters, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = Conv2D(num_filters, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "    return x\n",
    "\n",
    "def build_resnet50_unet(input_shape, n_classes):\n",
    "    inputs = Input(input_shape)\n",
    "    x = Lambda(lambda x: tf.repeat(x, 3, axis=-1))(inputs)\n",
    "    base_model = ResNet50(input_shape=(input_shape[0], input_shape[1], 3), include_top=False, weights='imagenet', input_tensor=x)\n",
    "\n",
    "    # Encoder part of the architecture (ResNet50 model)\n",
    "    s1 = base_model.get_layer(\"conv1_relu\").output                # 64 filters\n",
    "    s2 = base_model.get_layer(\"conv2_block3_out\").output          # 256 filters\n",
    "    s3 = base_model.get_layer(\"conv3_block4_out\").output          # 512 filters\n",
    "    s4 = base_model.get_layer(\"conv4_block6_out\").output          # 1024 filters\n",
    "    b1 = base_model.get_layer(\"conv5_block3_out\").output          # 2048 filters\n",
    "\n",
    "    # Decoder part of the architecture\n",
    "    d1 = decoder_block(b1, s4, 1024)  # Match number of filters with conv4 of encoder\n",
    "    d2 = decoder_block(d1, s3, 512)   # Match number of filters with conv3 of encoder\n",
    "    d3 = decoder_block(d2, s2, 256)   # Match number of filters with conv2 of encoder\n",
    "    d4 = decoder_block(d3, s1, 64)    # Match number of filters with conv1 of encoder\n",
    "\n",
    "    x = Conv2DTranspose(1, (2, 2), strides=(2, 2), padding='same')(d4)\n",
    "    x = Conv2D(1, (1, 1), padding=\"same\")(x)\n",
    "    \n",
    "    if n_classes == 1:  # Binary\n",
    "        activation = 'sigmoid'\n",
    "    else:\n",
    "        activation = 'softmax'\n",
    "    x = Activation(activation)(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the ResNet50-based model and compile it\n",
    "IMG_HEIGHT = image_dataset.shape[1]\n",
    "IMG_WIDTH  = image_dataset.shape[2]\n",
    "IMG_CHANNELS = image_dataset.shape[3]\n",
    "\n",
    "# Define and build model\n",
    "input_shape = (IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\n",
    "model = build_resnet50_unet(input_shape, n_classes=1)\n",
    "\n",
    "# Freeze the layers of the ResNet50 model\n",
    "for layer in model.layers[:175]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate = 1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define image and mask data generators with the desired augmentations\n",
    "data_gen_args = dict(rotation_range=20,\n",
    "                     width_shift_range=0.1,\n",
    "                     height_shift_range=0.1,\n",
    "                     shear_range=0.1,\n",
    "                     zoom_range=0.1,\n",
    "                     horizontal_flip=True,\n",
    "                     fill_mode='nearest')\n",
    "\n",
    "image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "mask_datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "#data generators\n",
    "def fit_generators(image_datagen, mask_datagen, images, masks, batch_size):\n",
    "    seed = 1\n",
    "    image_generator = image_datagen.flow(images, seed=seed, batch_size=batch_size)\n",
    "    mask_generator = mask_datagen.flow(masks, seed=seed, batch_size=batch_size)\n",
    "    return zip(image_generator, mask_generator)\n",
    "def train_and_visualize(model, images, masks, epochs, batch_size, n_splits=5):\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    fold = 0\n",
    "    history_list = []\n",
    "\n",
    "    for train_index, val_index in kfold.split(images, masks):\n",
    "        fold += 1\n",
    "        print(f\"Training fold {fold}\")\n",
    "\n",
    "        X_train, y_train = images[train_index], masks[train_index]\n",
    "        X_val, y_val = images[val_index], masks[val_index]\n",
    "\n",
    "        # Fit generators to the data\n",
    "        train_generator = fit_generators(image_datagen, mask_datagen, X_train, y_train, batch_size)\n",
    "        val_generator = fit_generators(image_datagen, mask_datagen, X_val, y_val, batch_size)\n",
    "\n",
    "        steps_per_epoch = len(X_train) // batch_size\n",
    "        validation_steps = len(X_val) // batch_size\n",
    "\n",
    "        history = model.fit(train_generator,\n",
    "                            steps_per_epoch=steps_per_epoch,\n",
    "                            epochs=epochs,\n",
    "                            validation_data=val_generator,\n",
    "                            validation_steps=validation_steps,\n",
    "                            shuffle=True)\n",
    "\n",
    "        history_list.append(history)\n",
    "\n",
    "        # Plot the training and validation loss for each fold\n",
    "        loss = history.history['loss']\n",
    "        val_loss = history.history['val_loss']\n",
    "        epochs_range = range(1, len(loss) + 1)\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs_range, loss, 'y', label='Training loss')\n",
    "        plt.plot(epochs_range, val_loss, 'r', label='Validation loss')\n",
    "        plt.title(f'Training and validation loss for fold {fold}')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        # Plot the training and validation accuracy for each fold\n",
    "        acc = history.history['accuracy']\n",
    "        val_acc = history.history['val_accuracy']\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs_range, acc, 'y', label='Training acc')\n",
    "        plt.plot(epochs_range, val_acc, 'r', label='Validation acc')\n",
    "        plt.title(f'Training and validation accuracy for fold {fold}')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    model_save_path = Path(\"\")\n",
    "    model.save(model_save_path)\n",
    "\n",
    "    return history_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the modified train_and_visualize function with the entire dataset\n",
    "train_and_visualize(model, image_dataset, mask_dataset, epochs=100, batch_size=16, n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section saves the model\n",
    "model.save(r\"c:\\Users\\richch\\Desktop\\VsCode\\SavedModels\\testEpoches.hdf5\")\n",
    "\n",
    "# This section loads the model\n",
    "from keras.models import load_model\n",
    "model_path = \"c:\\\\Users\\\\richch\\\\Desktop\\\\VsCode\\\\SavedModels\\\\testEpoches.hdf5\"\n",
    "mymodel = load_model(model_path)\n",
    "\n",
    "from keras.models import load_model\n",
    "model = load_model(r\"c:\\Users\\richch\\Desktop\\VsCode\\SavedModels\\testEpoches.hdf5\", compile=False)\n",
    "\n",
    "\n",
    "y_pred=model.predict(X_test)\n",
    "#print(X_test)\n",
    "print(y_test)\n",
    "\n",
    "\n",
    "y_pred_thresholded = y_pred > 0.5\n",
    "     \n",
    "from tensorflow.keras.metrics import MeanIoU\n",
    "     \n",
    "\n",
    "n_classes = 2\n",
    "IOU_keras = MeanIoU(num_classes=n_classes)  \n",
    "IOU_keras.update_state(y_pred_thresholded, y_test)\n",
    "print(\"Mean IoU =\", IOU_keras.result().numpy())\n",
    "\n",
    "\n",
    "threshold = 0.5\n",
    "test_img_number = random.randint(0, len(X_test)-1)\n",
    "test_img = X_test[test_img_number]\n",
    "ground_truth=y_test[test_img_number]\n",
    "test_img_input=np.expand_dims(test_img, 0)\n",
    "print(test_img_input.shape)\n",
    "prediction = (model.predict(test_img_input)[0,:,:,0] > 0.5).astype(np.uint8)\n",
    "print(prediction.shape)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(231)\n",
    "plt.title('Testing Image')\n",
    "plt.imshow(test_img[:,:,0], cmap='gray')\n",
    "plt.subplot(232)\n",
    "plt.title('Testing Label')\n",
    "plt.imshow(ground_truth[:,:,0], cmap='gray')\n",
    "plt.subplot(233)\n",
    "plt.title('Prediction on test image')\n",
    "plt.imshow(prediction, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "# Display the segmentation results for a random test image\n",
    "test_img_number = random.randint(0, len(X_test)-1)\n",
    "test_img = X_test[test_img_number]\n",
    "ground_truth = y_test[test_img_number]\n",
    "test_img_input = np.expand_dims(test_img, 0)\n",
    "prediction_autoencoder = (model.predict(test_img_input)[0, :, :, 0] > 0.5).astype(np.uint8)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(231)\n",
    "plt.title('Testing Image')\n",
    "plt.imshow(test_img[:,:,0], cmap='gray')\n",
    "plt.subplot(232)\n",
    "plt.title('Testing Label')\n",
    "plt.imshow(ground_truth[:,:,0], cmap='gray')\n",
    "plt.subplot(233)\n",
    "plt.title('Prediction on test image')\n",
    "plt.imshow(prediction_autoencoder, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "# Calculate the discrepancies between ground truth and prediction\n",
    "discrepancies = ground_truth[:,:,0] - prediction_autoencoder\n",
    "\n",
    "# Visualize the discrepancies\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(131)\n",
    "plt.title('Testing Image')\n",
    "plt.imshow(test_img[:,:,0], cmap='gray')\n",
    "plt.subplot(132)\n",
    "plt.title('Testing Label')\n",
    "plt.imshow(ground_truth[:,:,0], cmap='gray')\n",
    "plt.subplot(133)\n",
    "plt.title('Discrepancies (Ground Truth - Prediction)')\n",
    "plt.imshow(discrepancies, cmap='jet')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
